{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    from os import listdir\n",
    "    from os.path import join\n",
    "    import pickle\n",
    "    from queue import Empty, Queue\n",
    "    from math import exp\n",
    "    from math import log10\n",
    "    import torch.nn.functional as F\n",
    "    from matplotlib import pyplot as plt\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import random\n",
    "    import threading\n",
    "    from threading import Thread\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.autograd import Variable\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    from torchvision.models.vgg import vgg19\n",
    "    from tqdm import tqdm\n",
    "    from libs.SGDR import CosineAnnealingWarmRestarts\n",
    "\n",
    "except ImportError as e:\n",
    "    print(e)\n",
    "    raise ImportError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Sequential(*list(vgg19(pretrained=True).features)[:]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index_fname = 'data/data_index.p'\n",
    "data_index_val_fname = 'data/data_index_val.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(dataset_dir):\n",
    "    image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir)]\n",
    "    return image_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class thread_killer(object):    \n",
    "    \"\"\"\n",
    "    Boolean object for signaling a worker thread to terminate\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.to_kill = False\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.to_kill\n",
    "\n",
    "    def set_tokill(self, tokill):\n",
    "        self.to_kill = tokill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threaded_batches_feeder(tokill, batches_queue, dataset_generator):\n",
    "   \n",
    "    \"\"\"\n",
    "    Threaded worker for pre-processing input data.\n",
    "    tokill is a thread_killer object that indicates whether a thread should be terminated\n",
    "    dataset_generator is the training/validation dataset generator\n",
    "    batches_queue is a limited size thread-safe Queue instance.\n",
    "    \"\"\"\n",
    "    while tokill() == False:\n",
    "        for _, (batch_images, batch_targets, batch_mask) in enumerate(dataset_generator):\n",
    "            #We fill the queue with new fetched batch until we reach the max size.\n",
    "            batches_queue.put(((batch_images, batch_targets, batch_mask)), block=True)\n",
    "            if tokill() == True:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threaded_cuda_batches(tokill,cuda_batches_queue,batches_queue):\n",
    "    \n",
    "    \"\"\"\n",
    "    Thread worker for transferring pytorch tensors into GPU. \n",
    "    batches_queue is the queue that fetches numpy cpu tensors.\n",
    "    cuda_batches_queue receives numpy cpu tensors and transfers them to GPU space.\n",
    "    \"\"\"\n",
    "    while tokill() == False:\n",
    "        (batch_images, batch_targets, batch_masks) = batches_queue.get(block=True)\n",
    "        \n",
    "        batch_images = torch.from_numpy(batch_images)\n",
    "        batch_labels = torch.from_numpy(batch_targets)\n",
    "        batch_masks = torch.from_numpy(batch_masks)\n",
    "        \n",
    "        batch_images = Variable(batch_images).cuda()\n",
    "        batch_labels = Variable(batch_labels).cuda()\n",
    "        batch_masks = Variable(batch_masks).cuda()\n",
    "        \n",
    "        cuda_batches_queue.put(((batch_images, batch_labels, batch_masks)), block=True)\n",
    "        if tokill() == True:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class threadsafe_iter:\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_i(paths_count):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cyclic generator of paths indices\n",
    "    \"\"\"   \n",
    "    current_path_id = 0\n",
    "    while True:\n",
    "        yield current_path_id\n",
    "        current_path_id  = (current_path_id + 1) % paths_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGenerator:\n",
    "    def __init__(self, data_index_fname, batch_size, debug=False):\n",
    "        print('loading files index...')\n",
    "        \n",
    "        with open(data_index_fname, 'rb') as f:\n",
    "            data_dicts = pickle.load(f)\n",
    "            \n",
    "        lores_datafiles = [f for f in find_files('data/LoRes/Arrays/FIELD/')]\n",
    "        lores_datafiles.sort()\n",
    "        hires_datafiles = [f for f in find_files('data/HiRes/Arrays/FIELD/')]\n",
    "        hires_datafiles.sort()\n",
    "        \n",
    "        data_dict_lores = dict(zip(lores_datafiles, [np.load(f, mmap_mode = 'r') for f in lores_datafiles]))\n",
    "        data_dict_hires = dict(zip(hires_datafiles, [np.load(f, mmap_mode = 'r') for f in hires_datafiles]))\n",
    "        \n",
    "        self.data_index_fname = data_index_fname\n",
    "        self.data_mask = np.load('data/HiRes/Arrays/Percentile/Mask_95.npy')\n",
    "        self.batch_size = batch_size\n",
    "        self.debug = debug\n",
    "        self.index = 0\n",
    "        self.init_count = 0\n",
    "        self.dicts = data_dicts\n",
    "        self.dict_lores = data_dict_lores\n",
    "        self.dict_hires = data_dict_hires\n",
    "        print('examples number: %d' % len(self.dicts))\n",
    "\n",
    "        self.lock = threading.Lock()  # mutex for input path\n",
    "        self.yield_lock = threading.Lock()  # mutex for generator yielding of batch\n",
    "        self.path_id_generator = threadsafe_iter(get_path_i(len(self.dicts)))\n",
    "        self.imgs = []\n",
    "        self.trgs = []\n",
    "        self.msks = []\n",
    "\n",
    "    #def preprocess_img(self, fn):\n",
    "    #    img = None\n",
    "    #    if self.args.memcache:\n",
    "    #        if fn in self.images_cached:\n",
    "    #            img = self.images_cached[fn]\n",
    "    #    if img is None:\n",
    "    #        img = cv2.imread(fn)\n",
    "    #        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #        if self.args.memcache:\n",
    "    #            self.images_cached[fn] = img\n",
    "     #   return img\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.dicts)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            # In the start of each epoch we shuffle the data paths\n",
    "            with self.lock:\n",
    "                if (self.init_count == 0):\n",
    "                    self.shuffle()\n",
    "                    self.imgs, self.trgs, self_msks = [], [], []\n",
    "                    self.init_count = 1\n",
    "            # Iterates through the input paths in a thread-safe manner\n",
    "            for path_id in self.path_id_generator:\n",
    "                \n",
    "                img_fn = self.dicts[path_id][\"data_fname_lores\"]\n",
    "                img_id = self.dicts[path_id][\"data_lores_idx\"]\n",
    "                img = self.dict_lores[img_fn][img_id]\n",
    "                \n",
    "                trg_fn = self.dicts[path_id][\"data_fname_hires\"]\n",
    "                trg_id = self.dicts[path_id][\"data_hires_idx\"]\n",
    "                trg = self.dict_hires[trg_fn][trg_id]\n",
    "                \n",
    "                msk_id = self.dicts[path_id]['season_idx']\n",
    "                msk = self.data_mask[msk_id]\n",
    "\n",
    "                #img = self.preprocess_img(img_fn)\n",
    "                #trg = self.targets_transformer(trgs)\n",
    "                #trg = trg.astype(np.float32)\n",
    "\n",
    "                # Concurrent access by multiple threads to the lists below\n",
    "                with self.yield_lock:\n",
    "                    if (len(self.imgs)) < self.batch_size:\n",
    "                        self.imgs.append(img)\n",
    "                        self.trgs.append(trg)\n",
    "                        self.msks.append(msk)\n",
    "                    if len(self.imgs) % self.batch_size == 0:\n",
    "                        imgs_f32 = np.float32(self.imgs)\n",
    "                        trgs_f32 = np.float32(self.trgs)\n",
    "                        msks_f32 = np.float32(self.msks)\n",
    "                        yield (imgs_f32, trgs_f32, msks_f32)\n",
    "                        self.imgs, self.trgs, self.msks = [], [], []\n",
    "            # At the end of an epoch we re-init data-structures\n",
    "            with self.lock:\n",
    "                random.shuffle(self.dicts)\n",
    "                self.init_count = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.__iter__()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "    return gauss / gauss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    #.mm - Performs a matrix multiplication of the matrice\n",
    "    \n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(img1, img2, window_size=110, size_average=True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        #vgg = vgg19(pretrained=True)\n",
    "        #vgg = vgg.cuda()\n",
    "        #loss_network = nn.Sequential(*list(vgg.features)[:20]).eval()\n",
    "        # A sequential container. Modules will be added to it in the order they are passed in the constructor. \n",
    "        # Alternatively, an ordered dict of modules can also be passed in.\n",
    "        #for param in loss_network.parameters():\n",
    "        #    param.requires_grad = False\n",
    "        #self.loss_network = loss_network\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, fake_labels, out_images, target_images):\n",
    "        # Adversarial Loss\n",
    "        adversarial_loss = torch.mean(1 - fake_labels)\n",
    "        \n",
    "        # Perception Loss\n",
    "        #perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
    "        \n",
    "        # Image Loss\n",
    "        image_loss = self.mse_loss(out_images, target_images)\n",
    "        \n",
    "        #generator_loss = image_loss + 0.005 * adversarial_loss + 0.01 * perception_loss\n",
    "        generator_loss = image_loss + 0.005 * adversarial_loss\n",
    "        return generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class DiscriminatorLoss(nn.Module):\n",
    "    #def __init__(self):\n",
    "    #    super(DiscriminatorLoss, self).__init__()\n",
    "\n",
    "    #def forward(self, fake_labels, real_labels):\n",
    "    #    discriminator_loss = 1 - real_labels + fake_labels\n",
    "    #    return discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss = GeneratorLoss()\n",
    "#d_loss = DiscriminatorLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=9, padding=4),nn.PReLU())\n",
    "        self.block2 = ResidualBlock(64)\n",
    "        self.block3 = ResidualBlock(64)\n",
    "        self.block4 = ResidualBlock(64)\n",
    "        self.block5 = ResidualBlock(64)\n",
    "        self.block6 = ResidualBlock(64)\n",
    "        self.block7 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1),nn.BatchNorm2d(64))\n",
    "        self.block8 = UpsampleBLock(64, 5)\n",
    "        self.block9 = nn.Conv2d(64, 3, kernel_size=9, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "        block6 = self.block6(block5)\n",
    "        block7 = self.block7(block6)\n",
    "        block8 = self.block8(block1 + block7)\n",
    "        block9 = self.block9(block8)\n",
    "\n",
    "        return (torch.tanh(block9) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1024, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return torch.sigmoid(self.net(x).view(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleBLock(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super(UpsampleBLock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
    "        \n",
    "        # Rearranges elements in a tensor of shape (∗,C×r^2,H,W) \n",
    "        # to a tensor of shape (*, C, H x r, W x r).\n",
    "        # This is useful for implementing efficient sub-pixel convolution with a stride of 1/r\n",
    "        \n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.prelu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netG = Generator()\n",
    "#print('# generator parameters:', sum(param.numel() for param in netG.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netD = Discriminator()\n",
    "#print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(netG: nn.Module, netD: nn.Module, optimizerG: torch.optim.Optimizer, optimizerD: torch.optim.Optimizer, \n",
    "                epoch: int, NUM_EPOCHS: int, cuda_batches_queue: Queue,\n",
    "                generator_criterion: callable, \n",
    "                #discriminator_criterion: callable,\n",
    "                STEPS_PER_EPOCH: int):\n",
    "    \n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    term_columns = os.get_terminal_size().columns\n",
    "    pbar = tqdm(total=STEPS_PER_EPOCH, ncols=min(term_columns,180))\n",
    "    \n",
    "    mse_metrics = 0.0\n",
    "    rmse_wind_metrics = 0.0\n",
    "    rmse_slp_metrics = 0.0\n",
    "    rmse95_metrics = 0.0\n",
    "    \n",
    "    ssim_metrics = 0.0\n",
    "    psnr_metrics = 0.0\n",
    "    #batch_sizes = 0\n",
    "    running_results_g_loss = 0.0\n",
    "    running_results_d_loss = 0.0\n",
    "    ssims = 0\n",
    "\n",
    "    for batch_idx in range(STEPS_PER_EPOCH):\n",
    "        \n",
    "        (img, trg, msk) = cuda_batches_queue.get(block=True)\n",
    "        \n",
    "        #batch_sizes += batch_size\n",
    "        \n",
    "        wind_real = torch.sqrt(torch.square(trg[:,0,:,:]) + torch.square(trg[:,1,:,:]))\n",
    "        \n",
    "        img_norm = torch.zeros_like(img)\n",
    "        trg_norm = torch.zeros_like(trg)\n",
    "        \n",
    "        img_norm[:,0,:,:] = (img[:,0,:,:]+40)/80\n",
    "        img_norm[:,1,:,:] = (img[:,1,:,:]+40)/80\n",
    "        img_norm[:,2,:,:] = (img[:,2,:,:]-940)/120\n",
    "                                \n",
    "        trg_norm[:,0,:,:] = (trg[:,0,:,:]+40)/80\n",
    "        trg_norm[:,1,:,:] = (trg[:,1,:,:]+40)/80      \n",
    "        trg_norm[:,2,:,:] = (trg[:,2,:,:]-940)/120\n",
    "                \n",
    "        diff_msk = (wind_real - msk) > 0\n",
    "        wind_real_msk = wind_real * diff_msk\n",
    "        \n",
    "        g_update_first = True\n",
    "\n",
    "        fake_img = netG(img_norm)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        g_loss = generator_criterion(fake_out, fake_img, trg_norm)\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        g_loss.backward(retain_graph = True)\n",
    "        optimizerG.step()     \n",
    "    \n",
    "        real_out = netD(trg_norm).mean()\n",
    "        fake_out = netD(fake_img.detach()).mean()\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "\n",
    "        netD.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        running_results_g_loss += g_loss.item()\n",
    "        running_results_d_loss += d_loss.item()\n",
    "        \n",
    "        fake_img_unnorm = torch.zeros_like(fake_img)\n",
    "        \n",
    "        fake_img_unnorm[:,0,:,:] = 80*fake_img[:,0,:,:] - 40\n",
    "        fake_img_unnorm[:,1,:,:] = 80*fake_img[:,1,:,:] - 40\n",
    "        fake_img_unnorm[:,2,:,:] = 120*fake_img[:,2,:,:] + 940\n",
    "        \n",
    "        wind_fake = torch.sqrt(torch.square(fake_img_unnorm[:,0,:,:]) + torch.square(fake_img_unnorm[:,1,:,:]))\n",
    "        wind_fake_msk = wind_fake * diff_msk\n",
    "        \n",
    "        batch_mse = ((fake_img - trg_norm) ** 2).data.mean()        \n",
    "        batch_rmse_wind = torch.sqrt(((wind_fake - wind_real) ** 2).data.mean())\n",
    "        batch_rmse95 = torch.sqrt(((wind_fake_msk - wind_real_msk) ** 2).data.sum() / diff_msk.data.sum())\n",
    "        batch_rmse_slp = torch.sqrt(((fake_img_unnorm[:,2,:,:] - trg[:,2,:,:]) ** 2).data.mean())\n",
    "        \n",
    "        mse_metrics += batch_mse        \n",
    "        rmse_wind_metrics += batch_rmse_wind\n",
    "        rmse_slp_metrics += batch_rmse_slp\n",
    "        rmse95_metrics += batch_rmse95\n",
    "        \n",
    "        batch_ssim = ssim(fake_img, trg_norm).item()\n",
    "        ssims += batch_ssim       \n",
    "        psnr_metrics = 10 * log10((trg_norm.max()**2) / (mse_metrics / (batch_idx+1)))\n",
    "        ssim_metrics = ssims / (batch_idx + 1)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix_str(\n",
    "            'Train Epoch: %d [%d/%d (%.2f%%)]\\tG_Loss: %.6f; D_Loss: %.6f; RMSE_Wind: %.6f; RMSE_SLP: %.6f; RMSE95: %.6f; SSIM: %.6f; PSNR: %.6f' % (epoch,\n",
    "                                                                                                                                                     batch_idx+1,\n",
    "                                                                                                                                                     STEPS_PER_EPOCH,\n",
    "                                                                                                                                                     100. *(batch_idx+1) / STEPS_PER_EPOCH,\n",
    "                                                                                                                                                     running_results_g_loss/(batch_idx + 1),\n",
    "                                                                                                                                                     running_results_d_loss/(batch_idx + 1),\n",
    "                                                                                                                                                     rmse_wind_metrics / (batch_idx + 1),\n",
    "                                                                                                                                                     rmse_slp_metrics / (batch_idx + 1),\n",
    "                                                                                                                                                     rmse95_metrics / (batch_idx + 1),\n",
    "                                                                                                                                                     ssim_metrics,\n",
    "                                                                                                                                                     psnr_metrics)\n",
    "        )\n",
    "        \n",
    "        if batch_idx >= STEPS_PER_EPOCH-1:\n",
    "            break\n",
    "            \n",
    "    g_loss = running_results_g_loss / STEPS_PER_EPOCH\n",
    "    d_loss = running_results_d_loss / STEPS_PER_EPOCH\n",
    "    \n",
    "    #ssim_metrics = float(((ssim_metrics / STEPS_PER_EPOCH)))\n",
    "    #psnr_metrics = float(((psnr_metrics / STEPS_PER_EPOCH)))\n",
    "    #mse_metrics = float((mse_metrics/STEPS_PER_EPOCH).cpu())\n",
    "    rmse_wind_metrics = float((rmse_wind_metrics/STEPS_PER_EPOCH).cpu())\n",
    "    rmse_slp_metrics = float((rmse_slp_metrics/STEPS_PER_EPOCH).cpu())\n",
    "    rmse95_metrics = float((rmse95_metrics/STEPS_PER_EPOCH).cpu())\n",
    "    \n",
    "    pbar.set_postfix_str(\n",
    "        'Train Epoch: %d; G_Loss: %.6f; D_Loss: %.6f; RMSE_Wind: %.6f; RMSE_SLP: %.6f; RMSE95: %.6f; SSIM: %.6f; PSNR: %.6f' % (epoch,\n",
    "                                                                                                                                g_loss,\n",
    "                                                                                                                                d_loss,\n",
    "                                                                                                                                rmse_wind_metrics,\n",
    "                                                                                                                                rmse_slp_metrics,\n",
    "                                                                                                                                rmse95_metrics,\n",
    "                                                                                                                                ssim_metrics,\n",
    "                                                                                                                                psnr_metrics)\n",
    "    )\n",
    " #   for tag, param in model.named_parameters():\n",
    "#        TBwriter.add_histogram('grad/%s'%tag, param.grad.data.cpu().numpy(), epoch)\n",
    " #       TBwriter.add_histogram('weight/%s' % tag, param.data.cpu().numpy(), epoch)\n",
    "\n",
    "    # if (args.debug & (epoch % 10 == 0)):\n",
    "    #     TBwriter.add_images('input', img, epoch)\n",
    "\n",
    "#    if args.debug:\n",
    "#        for mname in activations.keys():\n",
    "#            TBwriter.add_histogram('activations/%s' % mname, activations[mname], epoch)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    losses_dict = {'train_g_loss': g_loss,\n",
    "                   'train_d_loss': d_loss}\n",
    "    metrics_values = {'RMSE_Wind': rmse_wind_metrics,\n",
    "                      'RMSE_SLP': rmse_slp_metrics,\n",
    "                      'RMSE95': rmse95_metrics,\n",
    "                      'PSNR': psnr_metrics,\n",
    "                      'SSIM': ssim_metrics}\n",
    "    return dict(**losses_dict, **metrics_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(netG: nn.Module, netD: nn.Module, cuda_batches_queue: Queue, generator_criterion: callable,\n",
    "               VAL_STEPS: int, epoch: int):\n",
    "    \n",
    "    netG.eval()\n",
    "    netD.eval()\n",
    "    \n",
    "    mse_metrics = 0.0\n",
    "    rmse_wind_metrics = 0.0\n",
    "    rmse_slp_metrics = 0.0\n",
    "    rmse95_metrics = 0.0\n",
    "    ssim_metrics = 0.0\n",
    "    psnr_metrics = 0.0\n",
    "    #batch_sizes = 0\n",
    "    running_results_g_loss = 0.0\n",
    "    running_results_d_loss = 0.0\n",
    "    ssims = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        term_columns = os.get_terminal_size().columns\n",
    "        pbar = tqdm(total=VAL_STEPS, ncols=min(term_columns, 180))\n",
    "        for batch_idx in range(VAL_STEPS):\n",
    "            (img, trg, msk) = cuda_batches_queue.get(block=True)\n",
    "            \n",
    "            #batch_sizes += val_batch_size\n",
    "            \n",
    "            wind_real = torch.sqrt(torch.square(trg[:,0,:,:]) + torch.square(trg[:,1,:,:]))\n",
    "            \n",
    "            img_norm = torch.zeros_like(img)\n",
    "            trg_norm = torch.zeros_like(trg)\n",
    "\n",
    "            img_norm[:,0,:,:] = (img[:,0,:,:]+40)/80\n",
    "            img_norm[:,1,:,:] = (img[:,1,:,:]+40)/80\n",
    "            img_norm[:,2,:,:] = (img[:,2,:,:]-940)/120\n",
    "                                \n",
    "            trg_norm[:,0,:,:] = (trg[:,0,:,:]+40)/80\n",
    "            trg_norm[:,1,:,:] = (trg[:,1,:,:]+40)/80      \n",
    "            trg_norm[:,2,:,:] = (trg[:,2,:,:]-940)/120\n",
    "                \n",
    "            diff_msk = (wind_real - msk) > 0\n",
    "            wind_real_msk = wind_real * diff_msk\n",
    "\n",
    "            output = netG(img_norm)\n",
    "            fake_out = netD(output).mean()\n",
    "            g_loss = generator_criterion(fake_out, output, trg_norm)\n",
    "            real_out = netD(trg_norm).mean()\n",
    "            d_loss = 1 - real_out + fake_out\n",
    "\n",
    "            running_results_g_loss += g_loss.item()\n",
    "            running_results_d_loss += d_loss.item()\n",
    "            \n",
    "            output_unnorm = torch.zeros_like(output)\n",
    "        \n",
    "            output_unnorm[:,0,:,:] = 80*output[:,0,:,:] - 40\n",
    "            output_unnorm[:,1,:,:] = 80*output[:,1,:,:] - 40\n",
    "            output_unnorm[:,2,:,:] = 120*output[:,2,:,:] + 940\n",
    "            \n",
    "            wind_fake = torch.sqrt(torch.square(output_unnorm[:,0,:,:]) + torch.square(output_unnorm[:,1,:,:]))\n",
    "            wind_fake_msk = wind_fake * diff_msk\n",
    "            \n",
    "            batch_mse = ((output - trg_norm) ** 2).data.mean()        \n",
    "            batch_rmse_wind = torch.sqrt(((wind_fake - wind_real) ** 2).data.mean())\n",
    "            batch_rmse95 = torch.sqrt(((wind_fake_msk - wind_real_msk) ** 2).data.sum() / diff_msk.data.sum())\n",
    "            batch_rmse_slp = torch.sqrt(((output_unnorm[:,2,:,:] - trg[:,2,:,:]) ** 2).data.mean())\n",
    "            \n",
    "            mse_metrics += batch_mse        \n",
    "            rmse_wind_metrics += batch_rmse_wind\n",
    "            rmse_slp_metrics += batch_rmse_slp\n",
    "            rmse95_metrics += batch_rmse95\n",
    "            \n",
    "            batch_ssim = ssim(output, trg_norm).item()\n",
    "            ssims += batch_ssim       \n",
    "            psnr_metrics = 10 * log10((trg_norm.max()**2) / (mse_metrics / (batch_idx+1)))\n",
    "            ssim_metrics = ssims / (batch_idx + 1)\n",
    "        \n",
    "            pbar.update(1)\n",
    "            if batch_idx >= VAL_STEPS-1:\n",
    "                break\n",
    "        pbar.close()\n",
    "\n",
    "    g_loss = running_results_g_loss/VAL_STEPS\n",
    "    d_loss = running_results_d_loss/VAL_STEPS\n",
    "    \n",
    "    #mse_metrics = float((mse_metrics/VAL_STEPS).cpu())\n",
    "    rmse_wind_metrics = float((rmse_wind_metrics/VAL_STEPS).cpu())\n",
    "    rmse_slp_metrics = float((rmse_slp_metrics/VAL_STEPS).cpu())\n",
    "    rmse95_metrics = float((rmse95_metrics/VAL_STEPS).cpu())\n",
    "\n",
    "    losses_dict = {'val_g_loss': g_loss,\n",
    "                   'val_d_loss': d_loss}\n",
    "    metrics_values = {'RMSE_Wind': rmse_wind_metrics,\n",
    "                      'RMSE_SLP': rmse_slp_metrics,\n",
    "                      'RMSE95': rmse95_metrics,\n",
    "                      'PSNR': psnr_metrics,\n",
    "                      'SSIM': ssim_metrics}\n",
    "    return dict(**losses_dict, **metrics_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(start_epoch: int, NUM_EPOCHS: int, STEPS_PER_EPOCH: int, batch_size: int, VAL_STEPS: int, val_batch_size: int):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "   # resume_state = None\n",
    "   # if 'resume' in args:\n",
    "        # restore epoch and other parameters\n",
    "   #     with open(os.path.join('./', 'scripts_backup', args.resume, 'launch_parameters.txt'), 'r') as f:\n",
    "   #         args_resume = f.readlines()[1:]\n",
    "   #         args_resume = [t.replace('\\n', '') for t in args_resume]\n",
    "   #         args_resume = parse_args(args_resume)\n",
    "   #         for k in [k for k in args_resume.__dict__.keys()]:\n",
    "   #             if k in ['run_name', 'snapshot', 'resume', 'lr']:\n",
    "    #                continue\n",
    "                # if k in args.__dict__.keys():\n",
    "                #     continue\n",
    "    #            args.__dict__[k] = getattr(args_resume, k)\n",
    "\n",
    "    #    resume_state = SimpleNamespace()\n",
    "    #    resume_state.dates_train = np.load(os.path.join('./', 'scripts_backup', args.resume, 'dates_train.npy'), allow_pickle=True)\n",
    "    #    resume_state.dates_val = np.load(os.path.join('./', 'scripts_backup', args.resume, 'dates_val.npy'), allow_pickle=True)\n",
    "    #    resume_state.epoch_snapshot = find_files(os.path.join('./logs', args.resume), 'ep????.pth.tar')[0]\n",
    "    #    resume_state.epoch = int(os.path.basename(resume_state.epoch_snapshot).replace('.pth.tar', '').replace('ep', ''))\n",
    "    #    resume_state.lr = args.lr\n",
    "\n",
    "\n",
    "    #region args parsing\n",
    "    #curr_run_name = args.run_name\n",
    "    #endregion\n",
    "\n",
    "    #region preparations\n",
    "    #base_logs_dir = os.path.join('./logs', curr_run_name)\n",
    "    #try:\n",
    "    #    EnsureDirectoryExists(base_logs_dir)\n",
    "    #except:\n",
    "    #    print(f'logs directory couldn`t be found and couldn`t be created:\\n{base_logs_dir}')\n",
    "    #    raise FileNotFoundError(f'logs directory couldn`t be found and couldn`t be created:\\n{base_logs_dir}')\n",
    "\n",
    "    #scripts_backup_dir = os.path.join('./scripts_backup', curr_run_name)\n",
    "    #try:\n",
    "    #    EnsureDirectoryExists(scripts_backup_dir)\n",
    "    #except:\n",
    "    #    print(f'backup directory couldn`t be found and couldn`t be created:\\n{scripts_backup_dir}')\n",
    "    #    raise FileNotFoundError(f'backup directory couldn`t be found and couldn`t be created:\\n{scripts_backup_dir}')\n",
    "\n",
    "    #tboard_dir_train = os.path.join(os.path.abspath('./'), 'logs', curr_run_name, 'TBoard', 'train')\n",
    "    #tboard_dir_val = os.path.join(os.path.abspath('./'), 'logs', curr_run_name, 'TBoard', 'val')\n",
    "    #try:\n",
    "    #    EnsureDirectoryExists(tboard_dir_train)\n",
    "    #except:\n",
    "    #    print('Tensorboard directory couldn`t be found and couldn`t be created:\\n%s' % tboard_dir_train)\n",
    "    #    raise FileNotFoundError(\n",
    "    #        'Tensorboard directory directory couldn`t be found and couldn`t be created:\\n%s' % tboard_dir_train)\n",
    "    #try:\n",
    "    #    EnsureDirectoryExists(tboard_dir_val)\n",
    "    #except:\n",
    "    #    print('Tensorboard directory couldn`t be found and couldn`t be created:\\n%s' % tboard_dir_val)\n",
    "    #    raise FileNotFoundError(\n",
    "    #        'Tensorboard directory directory couldn`t be found and couldn`t be created:\\n%s' % tboard_dir_val)\n",
    "    #endregion\n",
    "\n",
    "    # region backing up the scripts configuration\n",
    "    #print('backing up the scripts')\n",
    "    #ignore_func = lambda dir, files: [f for f in files if (isfile(join(dir, f)) and f[-3:] != '.py')] + [d for d in files if ((isdir(d)) & (('srcdata' in d) |\n",
    "                                                                                                                                            #('scripts_backup' in d) |\n",
    "                                                                                                                                           # ('__pycache__' in d) |\n",
    "                                                                                                                                            #('.pytest_cache' in d) |\n",
    "                                                                                                                                            #d.endswith('.ipynb_checkpoints') |\n",
    "                                                                                                                                           # d.endswith('logs.bak') |\n",
    "                                                                                                                                           # d.endswith('outputs') |\n",
    "                                                                                                                                           # d.endswith('processed_data') |\n",
    "                                                                                                                                           # d.endswith('build') |\n",
    "                                                                                                                                           # d.endswith('logs') |\n",
    "                                                                                                                                           # d.endswith('snapshots')))]\n",
    "    #copytree_multi('./',\n",
    "    #               './scripts_backup/%s/' % curr_run_name,\n",
    "    #               ignore=ignore_func)\n",
    "\n",
    "    #with open(os.path.join(scripts_backup_dir, 'launch_parameters.txt'), 'w+') as f:\n",
    "    #    f.writelines([f'{s}\\n' for s in sys.argv])\n",
    "    # endregion backing up the scripts configuration\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    if cuda:\n",
    "        torch.cuda.set_device(0)\n",
    "    cuda_dev = torch.device('cuda:0')\n",
    "\n",
    "    print('creating the model')\n",
    "    \n",
    "    #if args.pnet:\n",
    "    #    model = SIAmodel_PyramidNet(args, classes_num=9)\n",
    "    #else:\n",
    "    #    model = SIAmodel(args, classes_num=9)\n",
    "    #if resume_state is not None:\n",
    "    #    model.load_state_dict(torch.load(resume_state.epoch_snapshot))\n",
    "\n",
    "    #TB_writer_train = SummaryWriter(log_dir=tboard_dir_train)\n",
    "    #TB_writer_val = SummaryWriter(log_dir=tboard_dir_val)\n",
    "\n",
    "    netG = Generator()\n",
    "    netG = netG.cuda()\n",
    "    \n",
    "    netD = Discriminator()\n",
    "    netD = netD.cuda()\n",
    "    \n",
    "    if start_epoch != 1:\n",
    "        netG.load_state_dict(torch.load('epochs/netG_epoch_%d.pth' % (start_epoch - 1)))\n",
    "        netD.load_state_dict(torch.load('epochs/netD_epoch_%d.pth' % (start_epoch - 1)))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    generator_criterion = GeneratorLoss()\n",
    "    #discriminator_criterion = DiscriminatorLoss()\n",
    "    \n",
    "    optimizerG = optim.Adam(netG.parameters(),lr=2e-4)\n",
    "    schedulerG = CosineAnnealingWarmRestarts(optimizerG, T_0=128, T_mult=2, eta_min=1.0e-9, lr_decay=0.75)\n",
    "    \n",
    "    optimizerD = optim.Adam(netD.parameters(),lr=2e-4)\n",
    "    schedulerD = CosineAnnealingWarmRestarts(optimizerD, T_0=128, T_mult=2, eta_min=1.0e-9, lr_decay=0.75)\n",
    "\n",
    "    #print('logging the graph of the model')\n",
    "    #TB_writer_train.add_graph(model, [torch.tensor(np.random.random(size=(args.batch_size, 3, args.img_size, args.img_size)).astype(np.float32)).cuda(),\n",
    "    #                                  torch.tensor(np.random.random(size=(args.batch_size, 3, args.img_size, args.img_size)).astype(np.float32)).cuda()])\n",
    "\n",
    "    #print('logging the summary of the model')\n",
    "    #with open(os.path.join(base_logs_dir, 'model_structure.txt'), 'w') as f:\n",
    "    #    with redirect_stdout(f):\n",
    "     #       summary(model,\n",
    "     #               x = torch.tensor(np.random.random(size=(args.batch_size, 3, args.img_size, args.img_size)).astype(np.float32)).cuda(),\n",
    "     #               msk = torch.tensor(np.random.random(size=(args.batch_size, 3, args.img_size, args.img_size)).astype(np.float32)).cuda())\n",
    "\n",
    "    #if args.model_only:\n",
    "    #    quit()\n",
    "\n",
    "    #region train dataset\n",
    "    # if resume_state is not None:\n",
    "    #     subsetting_option = resume_state.dates_val\n",
    "    # else:\n",
    "    #     subsetting_option = 0.75\n",
    "    \n",
    "    train_ds = InputGenerator(data_index_fname, batch_size, debug = False)\n",
    "    \n",
    "    batches_queue_length = min(STEPS_PER_EPOCH, 64)\n",
    "    \n",
    "    train_batches_queue = Queue(maxsize=batches_queue_length)\n",
    "    train_cuda_batches_queue = Queue(maxsize=4)\n",
    "    train_thread_killer = thread_killer()\n",
    "    train_thread_killer.set_tokill(False)\n",
    "    preprocess_workers = 4\n",
    "    \n",
    "    for _ in range(preprocess_workers):\n",
    "        thr = Thread(target=threaded_batches_feeder, args=(train_thread_killer, train_batches_queue, train_ds))\n",
    "        thr.start()\n",
    "    \n",
    "    train_cuda_transfers_thread_killer = thread_killer()\n",
    "    train_cuda_transfers_thread_killer.set_tokill(False)\n",
    "    train_cudathread = Thread(target=threaded_cuda_batches, args=(train_cuda_transfers_thread_killer, train_cuda_batches_queue, train_batches_queue))\n",
    "    train_cudathread.start()\n",
    "    #endregion train dataset\n",
    "\n",
    "    # region test dataset\n",
    "    val_ds = InputGenerator(data_index_val_fname, val_batch_size,debug=False)\n",
    "    \n",
    "    # dates_used_val = val_ds.dates_used\n",
    "    # np.save(os.path.join(scripts_backup_dir, 'dates_val.npy'), dates_used_val)\n",
    "    batches_queue_length = min(VAL_STEPS, 64)\n",
    "    \n",
    "    val_batches_queue = Queue(maxsize=batches_queue_length)\n",
    "    val_cuda_batches_queue = Queue(maxsize=4)\n",
    "    val_thread_killer = thread_killer()\n",
    "    val_thread_killer.set_tokill(False)\n",
    "    \n",
    "    for _ in range(preprocess_workers):\n",
    "        thr = Thread(target=threaded_batches_feeder, args=(val_thread_killer, val_batches_queue, val_ds))\n",
    "        thr.start()\n",
    "    val_cuda_transfers_thread_killer = thread_killer()\n",
    "    val_cuda_transfers_thread_killer.set_tokill(False)\n",
    "    val_cudathread = Thread(target=threaded_cuda_batches, args=(val_cuda_transfers_thread_killer, val_cuda_batches_queue, val_batches_queue))\n",
    "    val_cudathread.start()\n",
    "    # endregion train dataset\n",
    "\n",
    "\n",
    "\n",
    "    #ET = ElasticTransformer(img_size=(3,args.img_size,args.img_size),\n",
    "    #                        batch_size=args.batch_size,\n",
    "    #                        flow_initial_size=(args.img_size//32, args.img_size//32),\n",
    "    #                        flow_displacement_range=args.img_size/32)\n",
    "\n",
    "    #if args.model_type == 'PC':\n",
    "    #    def cross_entropy(pred, soft_targets):\n",
    "    #        log_softmax_pred = torch.nn.functional.log_softmax(pred, dim=1)\n",
    "    #        return torch.mean(torch.sum(- soft_targets * log_softmax_pred, 1))\n",
    "\n",
    "    #    loss_fn = cross_entropy\n",
    "    #elif args.model_type == 'OR':\n",
    "    #    loss_fn = F.binary_cross_entropy\n",
    "\n",
    "    #metric_equal = accuracy(name='accuracy', model_type=args.model_type, batch_size=args.batch_size)\n",
    "    #metric_leq1 = diff_leq_accuracy(name='leq1_accuracy', model_type=args.model_type, batch_size=args.batch_size, leq_threshold=1)\n",
    "\n",
    "\n",
    "    #region creating checkpoint writers\n",
    "    #val_loss_checkpointer = ModelsCheckpointer(model, 'ep%04d_valloss_%.6e.pth.tar', ['epoch', 'val_loss'],\n",
    "    #                                           base_dir = base_logs_dir, replace=True,\n",
    "    #                                           watch_metric_names=['val_loss'], watch_conditions=['min'])\n",
    "    #val_accuracy_checkpointer = ModelsCheckpointer(model, 'ep%04d_valacc_%.6e.pth.tar', ['epoch', 'accuracy'],\n",
    "    #                                               base_dir=base_logs_dir, replace=True,\n",
    "    #                                               watch_metric_names=['accuracy'], watch_conditions=['max'])\n",
    "    #val_leq1_accuracy_checkpointer = ModelsCheckpointer(model, 'ep%04d_valleq1acc_%.6e.pth.tar', ['epoch', 'leq1_accuracy'],\n",
    "    #                                                    base_dir=base_logs_dir, replace=True,\n",
    "    #                                                    watch_metric_names=['leq1_accuracy'], watch_conditions=['max'])\n",
    "    #mandatory_checkpointer = ModelsCheckpointer(model, 'ep%04d.pth.tar', ['epoch'], base_dir=base_logs_dir, replace=True)\n",
    "\n",
    "    #checkpoint_saver_final = ModelsCheckpointer(model, 'final.pth.tar', [], base_dir=base_logs_dir, replace=False)\n",
    "    #endregion\n",
    "\n",
    "\n",
    "\n",
    "    print('\\n\\nstart training')\n",
    "    #start_epoch = 1 if resume_state is None else resume_state.epoch\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS+1):\n",
    "    #print('\\n\\n%s: Train epoch: %d of %d' % (args.run_name, epoch, EPOCHS))\n",
    "    \n",
    "        print('Train epoch: %d of %d' % (epoch, NUM_EPOCHS))\n",
    "        train_metrics = train_epoch(netG, netD, optimizerG, optimizerD, epoch, NUM_EPOCHS, train_cuda_batches_queue,\n",
    "                                    generator_criterion, STEPS_PER_EPOCH)\n",
    "        print(str(train_metrics))\n",
    "        \n",
    "        print('\\nValidation:')\n",
    "        val_metrics = validation(netG, netD, val_cuda_batches_queue, generator_criterion,\n",
    "                                 VAL_STEPS, epoch)\n",
    "        print(str(val_metrics))\n",
    "\n",
    "        # note: this re-shuffling will not make an immediate effect since the queues are already filled with the\n",
    "        # examples from the previous shuffle-states of datasets\n",
    "        train_ds.shuffle()\n",
    "        val_ds.shuffle()\n",
    "        \n",
    "        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d.pth' % (epoch))\n",
    "        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d.pth' % (epoch))\n",
    "        #region checkpoints\n",
    "        #val_loss_checkpointer.save_models(pdict={'epoch': epoch, 'val_loss': val_metrics['val_loss']},\n",
    "        #                                  metrics=val_metrics)\n",
    "        #val_accuracy_checkpointer.save_models(pdict={'epoch': epoch, 'accuracy': val_metrics['accuracy']},\n",
    "        #                                      metrics=val_metrics)\n",
    "        #val_leq1_accuracy_checkpointer.save_models(pdict={'epoch': epoch, 'leq1_accuracy': val_metrics['leq1_accuracy']},\n",
    "        #                                           metrics=val_metrics)\n",
    "        #mandatory_checkpointer.save_models(pdict={'epoch': epoch})\n",
    "        #endregion\n",
    "\n",
    "        # region write losses to tensorboard\n",
    "        #TB_writer_train.add_scalar('g_loss', train_metrics['train_g_loss'], epoch)\n",
    "        #TB_writer_train.add_scalar('d_loss', train_metrics['train_d_loss'], epoch)\n",
    "        #TB_writer_train.add_scalar('LR', scheduler.get_last_lr()[-1], epoch)\n",
    "        #TB_writer_train.add_scalar('MSE', mse_metrics, epoch)\n",
    "        #TB_writer_train.add_scalar('SSIM', ssim_metrics, epoch)\n",
    "        #TB_writer_train.add_scalar('PSNR', psnr_metrics, epoch)\n",
    "\n",
    "        #TB_writer_val.add_scalar('accuracy', val_metrics['accuracy'], epoch)\n",
    "        #TB_writer_val.add_scalar('loss', val_metrics['val_loss'], epoch)\n",
    "        #TB_writer_val.add_scalar('leq1_accuracy', val_metrics['leq1_accuracy'], epoch)\n",
    "        # endregion\n",
    "        \n",
    "        schedulerD.step(epoch=epoch)\n",
    "        schedulerG.step(epoch=epoch)\n",
    "\n",
    "    #checkpoint_saver_final.save_models(None)\n",
    "\n",
    "\n",
    "    # train_ds.close()\n",
    "    # test_ds.close()\n",
    "    train_thread_killer.set_tokill(True)\n",
    "    train_cuda_transfers_thread_killer.set_tokill(True)\n",
    "    val_thread_killer.set_tokill(True)\n",
    "    val_cuda_transfers_thread_killer.set_tokill(True)\n",
    "    for _ in range(preprocess_workers):\n",
    "        try:\n",
    "            # Enforcing thread shutdown\n",
    "            train_batches_queue.get(block=True, timeout=1)\n",
    "            train_cuda_batches_queue.get(block=True, timeout=1)\n",
    "            val_batches_queue.get(block=True, timeout=1)\n",
    "            val_cuda_batches_queue.get(block=True, timeout=1)\n",
    "        except Empty:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(start_epoch = 1, NUM_EPOCHS=200, STEPS_PER_EPOCH=730, batch_size=8, VAL_STEPS=23375, val_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(field_hr[0,0,:,:]))\n",
    "print(np.max(field_hr[0,0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_lr_tensor = torch.from_numpy(field_lr[0,:,:,:]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_lr_tensor = field_lr_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_lr_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_lr_tensor[:,0,:,:] = (field_lr_tensor[:,0,:,:]+40)/80\n",
    "field_lr_tensor[:,1,:,:] = (field_lr_tensor[:,1,:,:]+40)/80\n",
    "field_lr_tensor[:,2,:,:] = (field_lr_tensor[:,2,:,:]-940)/120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_nn = netG(field_lr_tensor)\n",
    "field_nn = field_nn.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = (field_nn[0]*80)-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(u_nn.cpu().detach().numpy()))\n",
    "print(np.max(u_nn.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(u_nn.cpu().detach().numpy(), cmap = 'gray', vmin = -25, vmax = 25, origin ='lower')\n",
    "_ = plt.colorbar()\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(field_hr[0,0,:,:], cmap = 'gray', vmin = -25, vmax = 25, origin ='lower')\n",
    "_ = plt.colorbar()\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(field_lr[0,0,:,:], cmap = 'gray', vmin = -25, vmax = 25, origin ='lower')\n",
    "_ = plt.colorbar()\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(u_hr[0].flatten(), bins = 100)\n",
    "_ = plt.savefig(\"High.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(u_nn.cpu().detach().numpy().flatten(), bins = 100)\n",
    "_ = plt.savefig(\"NN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(field_lr[0][0].flatten(), bins = 100)\n",
    "_ = plt.savefig(\"Low.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
